{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7108f5ba",
   "metadata": {},
   "source": [
    "# Wav2Vec2BERT - 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db5a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import evaluate\n",
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "from transformers import (\n",
    "    Wav2Vec2BertForCTC,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2BertProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a073986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9433addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def display10(dataset):\n",
    "    for i in range(10):\n",
    "        r = random.randint(0,len(dataset))\n",
    "        print(i+1 , dataset[r]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e95636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and language\n",
    "CV_VERSION = \"mozilla-foundation/common_voice_16_0\"\n",
    "LANG_ID = \"hi\"  # Hindi\n",
    "\n",
    "# Base SSL model (wav2vec2-bert encoder)\n",
    "BASE_MODEL = \"facebook/w2v-bert-2.0\"\n",
    "\n",
    "# Audio parameters\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "\n",
    "# Training output dir\n",
    "OUTPUT_DIR = \"Ed-168/w2vbert-hi-ctc-cv16\"\n",
    "\n",
    "\n",
    "BATCH_SIZE          = 1      \n",
    "GRAD_ACCUM          = 16     \n",
    "LEARNING_RATE       = 3e-5  \n",
    "NUM_TRAIN_EPOCHS    = 40\n",
    "EVAL_STRATEGY       = \"steps\"\n",
    "EVAL_STEPS          = 1000   \n",
    "SAVE_STEPS          = 1000   \n",
    "LOGGING_STEPS       = 50\n",
    "WARMUP_RATIO        = 0.05\n",
    "FP16                = torch.cuda.is_available()      \n",
    "\n",
    "\n",
    "PUSH_TO_HUB = False\n",
    "HF_REPO_ID = \"Ed-168/w2vbert-hi-ctc-cv16\"  # e.g. \"username/w2vbert-hi-ctc-cv17\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac364673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\EDWIN\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\mozilla-foundation--common_voice_16_0\\3076bf9caad479bbd4fa71669eac459841567c9efac7e647db5ae1ef78abe82a (last modified on Sun Aug 10 17:11:43 2025) since it couldn't be found locally at mozilla-foundation/common_voice_16_0, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\EDWIN\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\mozilla-foundation--common_voice_16_0\\3076bf9caad479bbd4fa71669eac459841567c9efac7e647db5ae1ef78abe82a (last modified on Sun Aug 10 17:11:43 2025) since it couldn't be found locally at mozilla-foundation/common_voice_16_0, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# This will download and prepare the dataset (first run may take a while)\n",
    "from datasets import load_dataset\n",
    "\n",
    "common_voice_train = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_16_0\",\n",
    "    \"hi\",\n",
    "    split=\"train+validation\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_16_0\",\n",
    "    \"hi\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c944eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_SAMPLES = 1500\n",
    "NUM_TEST_SAMPLES = 750\n",
    "\n",
    "common_voice_train = common_voice_train.select(range(NUM_TRAIN_SAMPLES))\n",
    "common_voice_test = common_voice_test.select(range(NUM_TEST_SAMPLES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "391ef1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "print(len(common_voice_train))\n",
    "print(len(common_voice_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a22d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\" , \"variant\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\" , \"variant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af428282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice_train = common_voice_train.cast_column(\"audio\" , Audio(sampling_rate = 16000))\n",
    "common_voice_test = common_voice_test.cast_column(\"audio\" , Audio(sampling_rate = 16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29e1e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 फैशन की दुनिया में कामयाब होने के लिए अपनाए तरीके\n",
      "2 आप क्या पहनने वाले हैं?\n",
      "3 टॉम ने नीले कपड़े पहने थे।\n",
      "4 उसने मुझसे वह बात छिपाई।\n",
      "5 यूपी में मूर्ति, माफिया और मुल्जिमों की सरकार: नकवी\n",
      "6 उसने विस्तृत रूप से अपनी योजना समझाई।\n",
      "7 नोएडाः वेब वर्क कंपनी के दफ्तर पर पुलिस का छापा, अहम दस्तावेज बरामद\n",
      "8 टॉम बिलकुल हमारी तरह है।\n",
      "9 कोड़ा को न दी जाए जमानत: सीबीआई\n",
      "10 हॉकी मैच के दौरान स्टेडियम में मौजूद रहेंगे प्रधानमंत्री मनमोहन सिंह\n"
     ]
    }
   ],
   "source": [
    "display10(common_voice_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c8f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 इस दुर्घटना के लिए कौन ज़िम्मेदार है\n",
      "2 दिल्ली नॉर्थ एमसीडी में आर्थिक तंगी कमिश्नर को नहीं मिली तीन महीने से सैलरी\n",
      "3 छत्तीसगढ़ नक्सलगढ़ पर प्रहार का प्लान केंद्रीय गृह सचिव ने बुलाई उच्च स्तरीय बैठक\n",
      "4 वे विद्यार्थी कोरियाई हैं\n",
      "5 मैंने उसको पैसे देने की कोशिश करी पर उसने इनकार कर दिया\n",
      "6 वह अपने प्रयोगों में कबूतरों का उपयोग करता था\n",
      "7 ओम पुरी का निधन सलमान ने शेयर की ये खास तस्वीर\n",
      "8 एशिया में तीसरा सबसे बड़ा देश भारत है\n",
      "9 नई नीतियां अपनाएं बराक ओबामा ईरान\n",
      "10 मुझे नौवे महीने का नाम बताओ\n"
     ]
    }
   ],
   "source": [
    "# Define the regex at the top level so that subprocesses can access it\n",
    "chars_to_ignore_regex = r\"[\\\"\\'\\(\\)\\[\\]\\{\\}\\<\\>\\—\\–\\-\\—\\—\\–\\—\\.\\,\\?\\!\\:\\;\\।\\d\\@\\#\\$\\%\\^\\&\\*\\+\\=\\_\\\\\\/\\|~`]+\"\n",
    "\n",
    "def normalize_text(batch):\n",
    "    text = batch[\"sentence\"]\n",
    "    text = text.lower()\n",
    "    text = re.sub(chars_to_ignore_regex, \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    batch[\"sentence\"] = text\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(normalize_text)\n",
    "common_voice_test = common_voice_test.map(normalize_text)\n",
    "display10(common_voice_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "066a262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 97\n",
      "Sample of vocab keys: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ']\n",
      "Saved vocab to: Ed-168/w2vbert-hi-ctc-cv16\\vocab.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    return {\"all_text\": [all_text]}\n",
    "\n",
    "vocabs = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, remove_columns=common_voice_train.column_names)\n",
    "all_text = \" \".join(vocabs[\"all_text\"])\n",
    "vocab_list = sorted(list(set(list(all_text))))\n",
    "\n",
    "if \" \" in vocab_list:\n",
    "    vocab_list.remove(\" \")\n",
    "\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"|\"] = len(vocab_dict) \n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab_dict))\n",
    "print(\"Sample of vocab keys:\", list(vocab_dict.keys())[:60])\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "vocab_path = os.path.join(OUTPUT_DIR, \"vocab.json\")\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved vocab to:\", vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "262aa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor saved to: Ed-168/w2vbert-hi-ctc-cv16\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for CTC\n",
    "\n",
    "from transformers import SeamlessM4TFeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_path,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\",\n",
    ")\n",
    "\n",
    "\n",
    "feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2BertProcessor\n",
    "\n",
    "processor = Wav2Vec2BertProcessor(feature_extractor=feature_extractor , tokenizer=tokenizer)\n",
    "\n",
    "# Save processor for later use/inference\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Processor saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab13feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: बिजली बिल मुद्दे पर अरविंद केजरीवाल करेंगे अनशन\n",
      "Input array shape: (77184,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_clip = random.randint(0 , len(common_voice_train) -1 )\n",
    "print(\"Target text:\", common_voice_train[rand_clip][\"sentence\"])\n",
    "print(\"Input array shape:\", common_voice_train[rand_clip][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", common_voice_train[rand_clip][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c7e29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0d8bec11154400b6687c2d4ff3d0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551698fdbb9b453f89dcc9877ef7b86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example lengths: 2 23\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch['audio']\n",
    "    batch['input_features'] = processor(audio['array'] , sampling_rate=audio['sampling_rate'])\n",
    "    batch[\"input_length\"] = len(batch['input_features'])\n",
    "\n",
    "    batch['labels'] = processor(text = batch['sentence']).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(prepare_dataset , remove_columns=common_voice_train.column_names)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset , remove_columns = common_voice_test.column_names)\n",
    "\n",
    "print(\"Example lengths:\", len(common_voice_train[0][\"input_features\"]), len(common_voice_train[0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8719d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from dataclasses import dataclass, field\n",
    "# from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2BertProcessor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed2b7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c7b0793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\EDWIN\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--wer\\e41eaa77ca7152430cd94704de20946c1b004b5b488ab5d20b26fb81c6c15506 (last modified on Sun Aug 10 22:24:10 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # pred.predictions is float logits of shape (batch, time, vocab_size)\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.from_numpy(pred_logits).argmax(-1)\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    # Replace -100 with pad_token_id for decoding refs\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24fd2405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForCTC were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2BertForCTC\n",
    "\n",
    "model = Wav2Vec2BertForCTC.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bf1833d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Wav2Vec2BertForCTC' object has no attribute 'freeze_feature_extractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze_feature_extractor\u001b[49m()\n",
      "File \u001b[1;32mc:\\Users\\EDWIN\\OneDrive\\Documents\\GitHub\\Multilingual-ASR\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1962\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1962\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1964\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Wav2Vec2BertForCTC' object has no attribute 'freeze_feature_extractor'"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForCTC were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 580,589,470 | Trainable: 580,589,470\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the CTC head on top of wav2vec2-bert encoder\n",
    "# model = Wav2Vec2BertForCTC.from_pretrained(\n",
    "#     BASE_MODEL,\n",
    "#     vocab_size=len(processor.tokenizer),\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     ctc_loss_reduction=\"mean\",\n",
    "#     # You can set this to True for long-form training stability with LayerDrop models\n",
    "#     # but w2v-bert-2.0 doesn't use LayerDrop by default.\n",
    "# )\n",
    "\n",
    "# # Make sure the model knows the correct special tokens\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = len(processor.tokenizer)\n",
    "# model.to(device)\n",
    "\n",
    "# # Optionally freeze the feature encoder for a few epochs if you have small compute\n",
    "# # (uncomment to try). Often helps stabilize early training.\n",
    "# # if hasattr(model, \"freeze_feature_encoder\"):\n",
    "# #     model.freeze_feature_encoder()\n",
    "\n",
    "# # Print parameter count\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Total params: {total_params:,} | Trainable: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EDWIN\\AppData\\Local\\Temp\\ipykernel_27052\\1816509248.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer is ready.\n"
     ]
    }
   ],
   "source": [
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=FP16,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"none\"],  \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,  # ensures padding works\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ee22034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.rename_column(\"input_features\", \"input_values\")\n",
    "# common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d89895e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'input_length', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c2356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2520' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2520/2520 34:45:08, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>24.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>18.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>14.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>13.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>12.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>12.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>11.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>11.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>11.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>10.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>9.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>9.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>8.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>8.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>8.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>7.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>7.493700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>7.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>6.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>6.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>5.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>5.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>5.598100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>5.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>5.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>5.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.126200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "print(\"Training complete. Model and processor saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(common_voice_train)\n",
    "metrics[\"num_epochs\"] = training_args.num_train_epochs  \n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(f\"Training complete! Model and processor saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2BertForCTC, Wav2Vec2BertProcessor\n",
    "\n",
    "from transformers import Wav2Vec2BertProcessor\n",
    "processor = Wav2Vec2BertProcessor.from_pretrained(\"w2vbert-hi-ctc-cv16\")\n",
    "\n",
    "\n",
    "# # Load your fine-tuned model and processor\n",
    "model_dir = \"w2vbert-hi-ctc-cv16\"\n",
    "model = Wav2Vec2BertForCTC.from_pretrained(model_dir)\n",
    "# processor = Wav2Vec2BertProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174c505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test audio clip to: test_hindi_clip.wav\n",
      "Transcript: जानें उत्तर प्रदेश में किस-किस की है मुस्लिम वोटों पर नजर\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "# Load only a small sample from the Common Voice Hindi test set\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_16_0\", \"hi\", split=\"test\")\n",
    "\n",
    "# Select one example (e.g., first sample)\n",
    "sample = common_voice_test[10]\n",
    "\n",
    "# Access the audio array and sample rate for that example\n",
    "audio_array = sample[\"audio\"][\"array\"]\n",
    "sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# Save the audio to a WAV file locally\n",
    "output_wav_path = \"test_hindi_clip.wav\"\n",
    "sf.write(output_wav_path, audio_array, samplerate=sampling_rate)\n",
    "\n",
    "print(f\"Saved test audio clip to: {output_wav_path}\")\n",
    "print(\"Transcript:\", sample[\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc78baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "tensor([[91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "         91, 91, 91, 91, 91, 91, 91, 91]])\n",
      "Transcription: [PAD]\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"test_hindi_clip.wav\"\n",
    "\n",
    "# Load and preprocess audio\n",
    "speech, sr = librosa.load(audio_path, sr=processor.feature_extractor.sampling_rate)\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(sampling_rate)\n",
    "# Preprocess input for the model\n",
    "inputs = processor(audio=speech, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "print(predicted_ids)\n",
    "# Decode to text (Hindi)\n",
    "transcription = processor.batch_decode(predicted_ids)[0]\n",
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14531fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2BertForCTC(\n",
       "  (wav2vec2_bert): Wav2Vec2BertModel(\n",
       "    (feature_projection): Wav2Vec2BertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2BertEncoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2BertEncoderLayer(\n",
       "          (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn1): Wav2Vec2BertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn): Wav2Vec2BertSelfAttention(\n",
       "            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (distance_embedding): Embedding(73, 64)\n",
       "          )\n",
       "          (conv_module): Wav2Vec2BertConvolutionModule(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (glu): GLU(dim=1)\n",
       "            (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n",
       "            (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): SiLU()\n",
       "            (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn2): Wav2Vec2BertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=94, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a395fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 5 samples...\n",
      "REF: इस जीत के बाद भी नहीं मुस्कुराता तो लोग एबनॉर्मल कहतेः विराट कोहली\n",
      "HYP: [PAD]\n",
      "--------------------------------------------------------------------------------\n",
      "REF: इसे ठीक करना नामुमकिन है|\n",
      "HYP: [PAD]\n",
      "--------------------------------------------------------------------------------\n",
      "REF: मैं अपनी वर्तमान आमदनी से संतुष्ट हूँ।\n",
      "HYP: [PAD]\n",
      "--------------------------------------------------------------------------------\n",
      "REF: चमत्कारी हैं मां महालक्ष्मी\n",
      "HYP: [PAD]\n",
      "--------------------------------------------------------------------------------\n",
      "REF: मैं अपने बच्चों को टीवी नहीं देखने देता।\n",
      "HYP: [PAD]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ Demo WER on 5 samples: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Hindi Common Voice test split\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_16_0\",\n",
    "    \"hi\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Target sampling rate from processor\n",
    "TARGET_SAMPLING_RATE = processor.feature_extractor.sampling_rate\n",
    "\n",
    "# Initialize WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Take only 5 samples for demo\n",
    "sampled_test = common_voice_test.shuffle(seed=42).select(range(5))\n",
    "\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "print(\"Evaluating on 5 samples...\")\n",
    "for sample in sampled_test:\n",
    "    audio = sample[\"audio\"]\n",
    "    ref = sample[\"sentence\"]\n",
    "\n",
    "    # Preprocess audio\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_features.to(device)).logits\n",
    "\n",
    "    # Decode prediction\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    preds.append(pred)\n",
    "    refs.append(ref)\n",
    "\n",
    "    print(f\"REF: {ref}\\nHYP: {pred}\\n{'-'*80}\")\n",
    "\n",
    "# Compute WER\n",
    "demo_wer = wer_metric.compute(predictions=preds, references=refs)\n",
    "print(f\"\\n✅ Demo WER on 5 samples: {demo_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac8f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
